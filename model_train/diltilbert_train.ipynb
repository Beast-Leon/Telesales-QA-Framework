{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa579bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1bc8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create customzied dataset\n",
    "# class Dataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_values, labels = self.df[['input_values', 'score']].iloc[idx]\n",
    "#         return dict(\n",
    "#             input_values = torch.tensor(input_values),\n",
    "#             labels = torch.tensor(labels).float(),\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5cd45b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no problem i will send you the information sho...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maybe i give you a call back. do you prefer to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>okay thank you so much for your time.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>then okay i will call again on tuesday bye bye.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sure I can call you back on next tuesday. than...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  no problem i will send you the information sho...      2\n",
       "1  maybe i give you a call back. do you prefer to...      1\n",
       "2              okay thank you so much for your time.      2\n",
       "3    then okay i will call again on tuesday bye bye.      1\n",
       "4  sure I can call you back on next tuesday. than...      2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"../data_collection/sentiment_data.csv\")\n",
    "df = df.dropna()\n",
    "df['score'] = df['score'].apply(lambda x: x + 1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cb7f569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Souvikcmsa/SentimentAnalysisDistillBERT\")\n",
    "# Load auto tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Souvikcmsa/SentimentAnalysisDistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "62ab9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_text, test_text, train_label, test_label = train_test_split(df['text'], df['score'], test_size = 0.33, random_state = 1070, stratify = df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e2dde7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14                                           I hate you\n",
       "12    as a result of that insurance company actually...\n",
       "2                 okay thank you so much for your time.\n",
       "33            has anyone from us or a broker quoted you\n",
       "6     ya hello good afternoon speak to leon please. ...\n",
       "3       then okay i will call again on tuesday bye bye.\n",
       "30    may I put you on hold while I check with my su...\n",
       "1     maybe i give you a call back. do you prefer to...\n",
       "7     ya you see even just now he message me he said...\n",
       "9     good evening may i speak to mister leon please...\n",
       "27    it can protect yourself and your baby right al...\n",
       "20    okay very dangerous a lot of people actually g...\n",
       "0     no problem i will send you the information sho...\n",
       "26    I will definitely send you a brouchure then ca...\n",
       "24    now third party liability means touch wood in ...\n",
       "4     sure I can call you back on next tuesday. than...\n",
       "32      may I proceed to help you with the purchase now\n",
       "25    so they will definitely sue you for the damage...\n",
       "34      mr Ang this is how the protection works for you\n",
       "22    content you know everything inside your home l...\n",
       "21    but under our plan this will cover you up to o...\n",
       "15    miss seah what so special about I thirty is th...\n",
       "5     hi this is leon calling from income ntuc. is i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "70e8b400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     currently we partnership with abcd. can i chec...\n",
       "13                         i dont want to talk with you\n",
       "18    if fully paid you must have some form of cover...\n",
       "17                     okay your home is hdb or private\n",
       "11    ya hello good afternoon speak to nanny seah pl...\n",
       "16    if you donâ€™t mind me asking have you bought a ...\n",
       "28    as long as children below twenty five years ol...\n",
       "29    usually financially dependent on parents right...\n",
       "10    thank you for calling income. this is leon how...\n",
       "19    once they pay okay you are not really required...\n",
       "23    then most important is the reason main reason ...\n",
       "31                       thank you for holding the line\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6ba14d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f27ff8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ee7e1f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_length,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_length,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ef6326af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_label.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0af92515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5d6f1f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_classifier.weight not freezed\n",
      "True\n",
      "pre_classifier.bias not freezed\n",
      "True\n",
      "classifier.weight not freezed\n",
      "True\n",
      "classifier.bias not freezed\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Only freeze transformer part parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if name != \"pre_classifier.weight\" and name != \"pre_classifier.bias\" and name != \"classifier.weight\" and name != \"classifier.bias\":\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name, \"not freezed\")\n",
    "        print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a04cb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class disbert_arch(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(disbert_arch, self).__init__()\n",
    "        self.disbert = model\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    def forward(self, sent_id, mask):\n",
    "        x = self.disbert(sent_id, attention_mask = mask).logits\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8a1d15a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = disbert_arch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9c1eff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# optimizer from huggingface transformers\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(new_model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6437f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.83333333 0.63888889 0.85185185]\n"
     ]
    }
   ],
   "source": [
    "# Find Class Weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_wts = compute_class_weight(class_weight = 'balanced',\n",
    "                                 classes = np.unique(train_label),\n",
    "                                 y = train_label)\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0e89361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights = torch.tensor(class_wts, dtype = torch.float)\n",
    "\n",
    "# Loss function\n",
    "cross_entropy = nn.NLLLoss(weight = weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b0d891a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tune Distil Bert\n",
    "# function to train the model\n",
    "def train(model, train_dataloader = train_dataloader):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        #batch = [r.to(device) for r in batch]\n",
    "        batch = [r for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        print(loss)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        preds = preds.detach().numpy()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        # preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "985f7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(model, val_dataloader = test_dataloader):\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        # batch = [t.to(device) for t in batch]\n",
    "        batch = [t for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2a3fb6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "tensor(0.0179, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.018\n",
      "Validation Loss: 0.020\n",
      "\n",
      " Epoch 2 / 10\n",
      "tensor(0.0077, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.008\n",
      "Validation Loss: 0.021\n",
      "\n",
      " Epoch 3 / 10\n",
      "tensor(0.0193, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.019\n",
      "Validation Loss: 0.033\n",
      "\n",
      " Epoch 4 / 10\n",
      "tensor(0.0098, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.010\n",
      "Validation Loss: 0.043\n",
      "\n",
      " Epoch 5 / 10\n",
      "tensor(0.0201, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.020\n",
      "Validation Loss: 0.041\n",
      "\n",
      " Epoch 6 / 10\n",
      "tensor(0.0049, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.037\n",
      "\n",
      " Epoch 7 / 10\n",
      "tensor(0.0051, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.032\n",
      "\n",
      " Epoch 8 / 10\n",
      "tensor(0.0149, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.015\n",
      "Validation Loss: 0.021\n",
      "\n",
      " Epoch 9 / 10\n",
      "tensor(0.0140, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.014\n",
      "Validation Loss: 0.013\n",
      "\n",
      " Epoch 10 / 10\n",
      "tensor(0.0181, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.018\n",
      "Validation Loss: 0.014\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train(new_model)\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate(new_model)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(new_model.state_dict(), 'results/fine_tune_disbert.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "48ab946b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model\n",
    "path = 'results/fine_tune_disbert.pt'\n",
    "new_model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "05c07608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sentences\n",
    "samp_sen = [\"you suck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3a2a4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentence\n",
    "samp_id = tokenizer.batch_encode_plus(samp_sen, padding = True, return_token_type_ids = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d8dd9ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2017, 11891, 102]], 'attention_mask': [[1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0040f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the model\n",
    "samp_seq = torch.tensor(samp_id['input_ids'])\n",
    "samp_mask = torch.tensor(samp_id['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "20a0a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = new_model(samp_seq, samp_mask)\n",
    "    preds = preds.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fa3d082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d9099d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2022",
   "language": "python",
   "name": "venv2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
